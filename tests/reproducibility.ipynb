{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from mrsc.src.model.SVDmodel import SVDmodel\n",
    "from mrsc.src.model.Target import Target\n",
    "from mrsc.src.model.Donor import Donor\n",
    "from mrsc.src.synthcontrol.mRSC import mRSC\n",
    "from mrsc.src.importData import *\n",
    "import mrsc.src.utils as utils\n",
    "\n",
    "from itertools import combinations, product\n",
    "\n",
    "def prepareData(stats):\n",
    "    # transform stats to a dictionary composed of df's for each stat\n",
    "    # the stats are re-calculated to get one stat for each year\n",
    "    metricsPerGameColNames = [\"PTS\",\"AST\",\"TOV\",\"TRB\",\"STL\",\"BLK\",\"3P\"]\n",
    "    metricsPerGameDict = getMetricsPerGameDict(stats, metricsPerGameColNames)\n",
    "\n",
    "    metricsPerCentColNames = [\"FG\",\"FT\"]\n",
    "    metricsPerCentDict = getMetricsPerCentDict(stats, metricsPerCentColNames)\n",
    "\n",
    "    metricsWeightedColNames = [\"PER\"]\n",
    "    metricsWeightedDict = getMetricsWeightedDict(stats, metricsWeightedColNames)\n",
    "\n",
    "    allMetricsDict = {**metricsPerGameDict, **metricsPerCentDict, **metricsWeightedDict}\n",
    "    allPivotedTableDict = getPivotedTableDict(allMetricsDict)\n",
    "    allMetrics = list(allMetricsDict.keys())\n",
    "    return allPivotedTableDict, allMetrics\n",
    "\n",
    "def getActivePlayers(stats, year, buffer):\n",
    "    # list of name of the players who were active in this and last year\n",
    "    thisYear = stats[stats.Year == year].copy()\n",
    "    players = list(thisYear.Player.unique())\n",
    "    for i in range(1, buffer+1):\n",
    "        previousYear = stats[stats.Year == (year-i)].copy()\n",
    "        players = list(set(players) & set(previousYear.Player.unique()))\n",
    "    return players\n",
    "\n",
    "def getTopPlayers(stats, year, metric, n):\n",
    "    stats = stats[stats.Year == year]\n",
    "    stats = stats.groupby('Player').mean().reset_index()\n",
    "    stats_sorted = stats[stats.Year == year].sort_values(metric, ascending = False).reset_index(drop=True)\n",
    "    return stats_sorted[[\"Player\"]][:n]\n",
    "\n",
    "def getBenchmark(target, metrics_to_use, pred_interval):    \n",
    "    target_data, nanIndex = target.concat(metrics_to_use)\n",
    "    num_k = len(metrics_to_use)\n",
    "    interv_index = int(target_data.shape[1]/num_k - pred_interval)\n",
    "    total_index = int(interv_index + 1)\n",
    "    \n",
    "    # true\n",
    "    true = utils.get_postint_data(target_data, interv_index, total_index, num_k).T\n",
    "    true.index = metrics_to_use\n",
    "    \n",
    "    # predictions\n",
    "    history = utils.get_preint_data(target_data, interv_index, total_index, num_k)\n",
    "    pred = []\n",
    "    for i in range(num_k):\n",
    "        pred.append(history.iloc[:,i*interv_index:(i+1)*interv_index].mean(axis=1).to_list())\n",
    "\n",
    "    pred = pd.DataFrame(pred, index=metrics_to_use, columns = [playerName])\n",
    "    return true, pred\n",
    "\n",
    "def getR2(true, pred, bench):\n",
    "    true_mean = true.mean(axis=1).to_frame()\n",
    "    ss_res = pd.DataFrame((true.values - pred.values)**2, index=true.index).sum(axis=1)\n",
    "    ss_tot = pd.DataFrame((true.values - bench.values)**2, index=true.index).sum(axis=1)\n",
    "    return (1-ss_res/ss_tot).to_frame()\n",
    "\n",
    "# def get_r_squared(predicted, actual):\n",
    "#     ss_res = np.sum(np.square(actual - predicted))\n",
    "#     ss_tot = np.sum(np.square(actual - np.mean(actual)))\n",
    "#     return 1 - (ss_res / ss_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** importing data ***\n",
      "*** preparing data ***\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "import data\n",
    "\"\"\"\n",
    "pred_year = 2015 # the year that we are living in\n",
    "pred_interval = 1 # we are making predictions for pred_year+1 and +2\n",
    "# min_games = 40\n",
    "\n",
    "print(\"*** importing data ***\")\n",
    "players = pd.read_csv(\"../data/nba-players-stats/player_data.csv\")\n",
    "players = players[players.year_start >= 1980] # only choose players who started after 1980\n",
    "# players[\"player_id\"] = range(0,len(players.name)) # assign id\n",
    "\n",
    "stats = pd.read_csv(\"../data/nba-players-stats/Seasons_Stats.csv\")\n",
    "stats = stats[stats.Player.isin(players.name)]\n",
    "\n",
    "# only after 1980\n",
    "stats = stats[stats.Year >= 1980]\n",
    "# stats = stats[stats.G >= min_games]\n",
    "\n",
    "# without duplicated names --> to do: how to distinguish multiple player with the same name\n",
    "stats = removeDuplicated(players, stats)\n",
    "stats.Year = stats.Year.astype(int)\n",
    "stats.year_count = stats.year_count.astype(int)\n",
    "\n",
    "print(\"*** preparing data ***\")\n",
    "\n",
    "########### Donor ##########\n",
    "# filter stats by the year\n",
    "stats_donor = stats[stats.Year <= pred_year]\n",
    "allPivotedTableDict_d, allMetrics = prepareData(stats_donor)\n",
    "donor = Donor(allPivotedTableDict_d)\n",
    "\n",
    "########### Target ##########\n",
    "# filter stats by the year\n",
    "stats_target = stats[stats.Year <= pred_year+pred_interval]\n",
    "allPivotedTableDict, allMetrics = prepareData(stats_target)\n",
    "\n",
    "# just to debug\n",
    "df_year = pd.pivot_table(stats, values=\"Year\", index=\"Player\", columns = \"year_count\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "targets\n",
    "\"\"\"\n",
    "activePlayers = getActivePlayers(stats, pred_year+1, buffer=4)\n",
    "activePlayers.sort()\n",
    "activePlayers.remove(\"Kevin Garnett\")\n",
    "activePlayers.remove(\"Kobe Bryant\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Algo: outputs the mean of the player's history\n",
      "-----\n",
      "*** MAPE ***\n",
      "PTS_G    0.575711\n",
      "AST_G    0.506711\n",
      "TOV_G    0.596790\n",
      "TRB_G    0.422301\n",
      "STL_G    0.446710\n",
      "BLK_G    0.685350\n",
      "3P_G     0.682272\n",
      "FG%      0.076904\n",
      "FT%      0.081673\n",
      "PER_w    0.287966\n",
      "dtype: float64\n",
      "MAPE for all:  0.4420163650526481\n",
      "\n",
      "*** RMSE ***\n",
      "PTS_G    4.691799\n",
      "AST_G    1.136067\n",
      "TOV_G    0.617640\n",
      "TRB_G    1.709996\n",
      "STL_G    0.299601\n",
      "BLK_G    0.282306\n",
      "3P_G     0.503330\n",
      "FG%      0.052942\n",
      "FT%      0.110670\n",
      "PER_w    3.788750\n",
      "dtype: float64\n",
      "RMSE for all:  1.3193101146400346\n"
     ]
    }
   ],
   "source": [
    "# metrics_to_use= [\"PTS_G\",\"AST_G\",\"TOV_G\",\"PER_w\", \"FG%\",\"FT%\",\"3P_G\",\"TRB_G\",\"STL_G\",\"BLK_G\"]\n",
    "metrics_to_use = allMetrics\n",
    "\n",
    "print(\"Algo: outputs the mean of the player's history\")\n",
    "print(\"-----\")\n",
    "pred_all = pd.DataFrame()\n",
    "true_all = pd.DataFrame()\n",
    "for playerName in activePlayers:\n",
    "    target = Target(playerName, allPivotedTableDict)\n",
    "    true, pred = getBenchmark(target, metrics_to_use, pred_interval)\n",
    "    \n",
    "    pred_all = pd.concat([pred_all, pred], axis=1)\n",
    "    true_all = pd.concat([true_all, true], axis=1)\n",
    "\n",
    "###################\n",
    "mask = (true_all !=0 )\n",
    "mape = np.abs(pred_all - true_all) / true_all[mask]\n",
    "print(\"*** MAPE ***\")\n",
    "print(mape.mean(axis=1))\n",
    "print(\"MAPE for all: \", mape.mean().mean())\n",
    "\n",
    "rmse = utils.rmse_2d(true_all, pred_all)\n",
    "print()\n",
    "print(\"*** RMSE ***\")\n",
    "print(rmse)\n",
    "print(\"RMSE for all: \", rmse.mean())    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grouping - Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 metric at once\n",
      "(10, 228)\n",
      "*** MAPE ***\n",
      "PTS_G    0.330647\n",
      "AST_G    0.372639\n",
      "TOV_G    0.339674\n",
      "TRB_G    0.313756\n",
      "STL_G    0.320293\n",
      "BLK_G    0.523257\n",
      "3P_G     0.642541\n",
      "FG%      0.083542\n",
      "FT%      0.086549\n",
      "PER_w    0.241234\n",
      "dtype: float64\n",
      "MAPE for all:  0.32538716712421906\n",
      "\n",
      "*** RMSE ***\n",
      "PTS_G    3.212774\n",
      "AST_G    0.948374\n",
      "TOV_G    0.429059\n",
      "TRB_G    1.526153\n",
      "STL_G    0.261448\n",
      "BLK_G    0.202434\n",
      "3P_G     0.495202\n",
      "FG%      0.055732\n",
      "FT%      0.111807\n",
      "PER_w    3.655378\n",
      "dtype: float64\n",
      "RMSE for all:  1.0898361617314343\n",
      "\n",
      "*** R2 ***\n",
      "PTS_G   -3.060526e+01\n",
      "AST_G   -1.394150e+05\n",
      "TOV_G   -1.368096e+02\n",
      "TRB_G   -2.987561e+01\n",
      "STL_G   -6.843799e+01\n",
      "BLK_G   -2.027625e+01\n",
      "3P_G    -1.286444e+02\n",
      "FG%     -1.232705e+08\n",
      "FT%     -8.834818e+01\n",
      "PER_w   -2.295992e+02\n",
      "dtype: float64\n",
      "R2 for all:  -12341062.69028894\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "experiment setup\n",
    "\"\"\"\n",
    "# overall setup\n",
    "donorSetup= [None,\"sliding\", True]\n",
    "denoiseSetup = [\"SVD\", \"all\"]\n",
    "regression_method = \"pinv\"\n",
    "\n",
    "threshold = 0.97\n",
    "verbose = False\n",
    "\n",
    "metrics_list = [[x] for x in allMetrics]\n",
    "\n",
    "\"\"\"\n",
    "experiment\n",
    "\"\"\"\n",
    "print(\"1 metric at once\")\n",
    "\n",
    "all_pred = pd.DataFrame()\n",
    "all_true = pd.DataFrame()\n",
    "all_bench = pd.DataFrame()\n",
    "all_R2 = pd.DataFrame()\n",
    "for playerName in activePlayers:\n",
    "    # print(playerName)\n",
    "    # print(\"*** year - year_count matching for this player\")\n",
    "    # a = df_year[df_year.index == playerName]\n",
    "    # print(a.dropna(axis=1))\n",
    "\n",
    "    target = Target(playerName, allPivotedTableDict)\n",
    "    # print(\"*** target - total index: \", target.total_index)\n",
    "    # print(target.concat(metrics_list[1]))\n",
    "    \n",
    "    # benchmark\n",
    "    true, benchmark = getBenchmark(target, allMetrics, pred_interval)\n",
    "    \n",
    "    # prediction\n",
    "    mrsc = mRSC(donor, target, pred_interval, probObservation=1)\n",
    "    player_pred = pd.DataFrame()\n",
    "    player_true = pd.DataFrame()\n",
    "    for i in range(len(metrics_list)):\n",
    "        mrsc.fit_threshold(metrics_list[i], threshold, donorSetup, denoiseSetup,regression_method, verbose)\n",
    "        pred = mrsc.predict()\n",
    "        true = mrsc.getTrue()\n",
    "        pred.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "        true.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "        player_pred = pd.concat([player_pred, pred], axis=0)\n",
    "        player_true = pd.concat([player_true, true], axis=0)\n",
    "    all_pred = pd.concat([all_pred, player_pred], axis=1)\n",
    "    all_true = pd.concat([all_true, player_true], axis=1)\n",
    "    all_bench = pd.concat([all_bench, benchmark], axis=1)\n",
    "    \n",
    "    R2 = getR2(player_true, player_pred, benchmark)\n",
    "    all_R2 = pd.concat([all_R2, R2], axis=1)\n",
    "\n",
    "###################\n",
    "# print(all_pred)\n",
    "print(all_pred.shape)\n",
    "mask = (all_true !=0 )\n",
    "mape = np.abs(all_pred - all_true) / all_true[mask]\n",
    "print(\"*** MAPE ***\")\n",
    "print(mape.mean(axis=1))\n",
    "print(\"MAPE for all: \", mape.mean().mean())\n",
    "\n",
    "rmse = utils.rmse_2d(all_true, all_pred)\n",
    "print()\n",
    "print(\"*** RMSE ***\")\n",
    "print(rmse)\n",
    "print(\"RMSE for all: \", rmse.mean())\n",
    "\n",
    "print()\n",
    "print(\"*** R2 ***\")\n",
    "print(all_R2.mean(axis=1))\n",
    "print(\"R2 for all: \", all_R2.mean(axis=1).mean(axis=0))\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 metrics at once\n",
      "(10, 228)\n",
      "*** MAPE ***\n",
      "PTS_G    0.367120\n",
      "AST_G    0.598520\n",
      "TOV_G    0.392989\n",
      "TRB_G    0.346319\n",
      "STL_G    0.427427\n",
      "BLK_G    0.733845\n",
      "3P_G     1.204553\n",
      "FG%      0.092947\n",
      "FT%      0.100229\n",
      "PER_w    0.254007\n",
      "dtype: float64\n",
      "MAPE for all:  0.4420708928402392\n",
      "\n",
      "*** RMSE ***\n",
      "PTS_G    4.051636\n",
      "AST_G    1.236445\n",
      "TOV_G    0.569640\n",
      "TRB_G    1.684309\n",
      "STL_G    0.359940\n",
      "BLK_G    0.293392\n",
      "3P_G     0.550777\n",
      "FG%      0.062353\n",
      "FT%      0.119759\n",
      "PER_w    4.139142\n",
      "dtype: float64\n",
      "RMSE for all:  1.3067391912364323\n",
      "\n",
      "*** R2 ***\n",
      "PTS_G   -8.995938e+01\n",
      "AST_G   -1.904628e+04\n",
      "TOV_G   -1.782756e+03\n",
      "TRB_G   -9.665420e+01\n",
      "STL_G   -5.556138e+01\n",
      "BLK_G   -1.173255e+02\n",
      "3P_G             -inf\n",
      "FG%     -9.264038e+07\n",
      "FT%     -2.786185e+03\n",
      "PER_w   -8.764996e+02\n",
      "dtype: float64\n",
      "R2 for all:  -inf\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "experiment setup\n",
    "\"\"\"\n",
    "# overall setup\n",
    "donorSetup= [\"variance_batch\",\"sliding\", True]\n",
    "denoiseSetup = [\"SVD\", \"all\"]\n",
    "regression_method = \"pinv\"\n",
    "\n",
    "threshold = 0.97\n",
    "verbose = False\n",
    "\n",
    "metrics_list = [allMetrics]\n",
    "\n",
    "\"\"\"\n",
    "experiment\n",
    "\"\"\"\n",
    "print(\"10 metrics at once\")\n",
    "\n",
    "all_pred = pd.DataFrame()\n",
    "all_true = pd.DataFrame()\n",
    "all_bench = pd.DataFrame()\n",
    "all_R2 = pd.DataFrame()\n",
    "for playerName in activePlayers:\n",
    "    # print(playerName)\n",
    "    # print(\"*** year - year_count matching for this player\")\n",
    "    # a = df_year[df_year.index == playerName]\n",
    "    # print(a.dropna(axis=1))\n",
    "\n",
    "    target = Target(playerName, allPivotedTableDict)\n",
    "    # print(\"*** target - total index: \", target.total_index)\n",
    "    # print(target.concat(metrics_list[1]))\n",
    "    \n",
    "    # benchmark\n",
    "    true, benchmark = getBenchmark(target, allMetrics, pred_interval)\n",
    "    \n",
    "    # prediction\n",
    "    mrsc = mRSC(donor, target, pred_interval, probObservation=1)\n",
    "    player_pred = pd.DataFrame()\n",
    "    player_true = pd.DataFrame()\n",
    "    for i in range(len(metrics_list)):\n",
    "        mrsc.fit_threshold(metrics_list[i], threshold, donorSetup, denoiseSetup,regression_method, verbose)\n",
    "        pred = mrsc.predict()\n",
    "        true = mrsc.getTrue()\n",
    "        pred.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "        true.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "        player_pred = pd.concat([player_pred, pred], axis=0)\n",
    "        player_true = pd.concat([player_true, true], axis=0)\n",
    "    all_pred = pd.concat([all_pred, player_pred], axis=1)\n",
    "    all_true = pd.concat([all_true, player_true], axis=1)\n",
    "    all_bench = pd.concat([all_bench, benchmark], axis=1)\n",
    "    \n",
    "    R2 = getR2(player_true, player_pred, benchmark)\n",
    "    all_R2 = pd.concat([all_R2, R2], axis=1)\n",
    "\n",
    "###################\n",
    "# print(all_pred)\n",
    "print(all_pred.shape)\n",
    "mask = (all_true !=0 )\n",
    "mape = np.abs(all_pred - all_true) / all_true[mask]\n",
    "print(\"*** MAPE ***\")\n",
    "print(mape.mean(axis=1))\n",
    "print(\"MAPE for all: \", mape.mean().mean())\n",
    "\n",
    "rmse = utils.rmse_2d(all_true, all_pred)\n",
    "print()\n",
    "print(\"*** RMSE ***\")\n",
    "print(rmse)\n",
    "print(\"RMSE for all: \", rmse.mean())\n",
    "\n",
    "print()\n",
    "print(\"*** R2 ***\")\n",
    "print(all_R2.mean(axis=1))\n",
    "print(\"R2 for all: \", all_R2.mean(axis=1).mean(axis=0))\n",
    "##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
