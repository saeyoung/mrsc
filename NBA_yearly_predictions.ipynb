{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "sys.path.append(\"../..\")\n",
    "sys.path.append(\"..\")\n",
    "sys.path.append(os.getcwd())\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import copy\n",
    "import pickle\n",
    "\n",
    "from mrsc.src.model.SVDmodel import SVDmodel\n",
    "from mrsc.src.model.Target import Target\n",
    "from mrsc.src.model.Donor import Donor\n",
    "from mrsc.src.synthcontrol.mRSC import mRSC\n",
    "from mrsc.src.importData import *\n",
    "import mrsc.src.utils as utils\n",
    "\n",
    "from itertools import combinations, product\n",
    "\n",
    "def prepareData(stats):\n",
    "    # transform stats to a dictionary composed of df's for each stat\n",
    "    # the stats are re-calculated to get one stat for each year\n",
    "    metricsPerGameColNames = [\"PTS\",\"AST\",\"TOV\",\"TRB\",\"STL\",\"BLK\",\"3P\",\"MP\"]\n",
    "    metricsPerGameDict = getMetricsPerGameDict(stats, metricsPerGameColNames)\n",
    "\n",
    "    metricsPerCentColNames = [\"FG\",\"FT\"]\n",
    "    metricsPerCentDict = getMetricsPerCentDict(stats, metricsPerCentColNames)\n",
    "\n",
    "    metricsWeightedColNames = [\"PER\"]\n",
    "    metricsWeightedDict = getMetricsWeightedDict(stats, metricsWeightedColNames)\n",
    "\n",
    "    allMetricsDict = {**metricsPerGameDict, **metricsPerCentDict, **metricsWeightedDict}\n",
    "    allPivotedTableDict = getPivotedTableDict(allMetricsDict)\n",
    "    allMetrics = list(allMetricsDict.keys())\n",
    "    return allPivotedTableDict, allMetrics\n",
    "\n",
    "# I think there's a bug here\n",
    "def getActivePlayers(stats, year, buffer, min_games):\n",
    "    # list of name of the players who were active in this and last year\n",
    "    thisYear = stats[stats.Year == year].copy()\n",
    "    thisYear = thisYear[thisYear.G >= min_games]\n",
    "    players = list(thisYear.Player.unique())\n",
    "    for i in range(1, buffer+1):\n",
    "        previousYear = stats[stats.Year == (year-i)].copy()\n",
    "        players = list(set(players) & set(previousYear.Player.unique()))\n",
    "    return players\n",
    "\n",
    "def topPlayers(stats, year, metric, n):\n",
    "    # n = number of top players \n",
    "    stats = stats[stats.Year == year]\n",
    "    stats = stats.groupby('Player').mean().reset_index()\n",
    "    stats_sorted = stats[stats.Year == year].sort_values(metric, ascending = False).reset_index(drop=True)\n",
    "    return stats_sorted[[\"Player\",\"player_id\"]][:n]\n",
    "\n",
    "def getBenchmark(target, metrics_to_use, pred_interval):    \n",
    "    target_data, nanIndex = target.concat(metrics_to_use)\n",
    "    num_k = len(metrics_to_use)\n",
    "    interv_index = int(target_data.shape[1]/num_k - pred_interval)\n",
    "    total_index = int(interv_index + 1)\n",
    "    \n",
    "    # true\n",
    "    true = utils.get_postint_data(target_data, interv_index, total_index, num_k).T\n",
    "    true.index = metrics_to_use\n",
    "    \n",
    "    # predictions\n",
    "    history = utils.get_preint_data(target_data, interv_index, total_index, num_k)\n",
    "    pred = []\n",
    "    for i in range(num_k):\n",
    "        pred.append(history.iloc[:,i*interv_index:(i+1)*interv_index].mean(axis=1).to_list())\n",
    "\n",
    "    pred = pd.DataFrame(pred, index=metrics_to_use, columns = [playerName])\n",
    "    return true, pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_pred = pd.DataFrame()\n",
    "all_true = pd.DataFrame()\n",
    "all_bench = pd.DataFrame()\n",
    "all_R2 = pd.DataFrame()\n",
    "for playerName in playerNames:\n",
    "    target = Target(playerName, allPivotedTableDict)\n",
    "    \n",
    "    # benchmark\n",
    "    true, benchmark = getBenchmark(target, [\"PTS_G\"], pred_interval)\n",
    "    \n",
    "    # prediction\n",
    "    mrsc = mRSC(donor, target, pred_interval, probObservation=1)\n",
    "    player_pred = pd.DataFrame()\n",
    "    player_true = pd.DataFrame()\n",
    "    for i in range(len(metrics_list)):\n",
    "        mrsc.fit_threshold(metrics_list[i], threshold, donorSetup, denoiseSetup,regression_method, verbose)\n",
    "        pred = mrsc.predict()\n",
    "        true = mrsc.getTrue()\n",
    "        pred.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "        true.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "        player_pred = pd.concat([player_pred, pred], axis=0)\n",
    "        player_true = pd.concat([player_true, true], axis=0)\n",
    "    all_pred = pd.concat([all_pred, player_pred], axis=1)\n",
    "    all_true = pd.concat([all_true, player_true], axis=1)\n",
    "    all_bench = pd.concat([all_bench, benchmark], axis=1)\n",
    "    \n",
    "    R2 = getR2(player_true, player_pred, benchmark)\n",
    "    all_R2 = pd.concat([all_R2, R2], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_pred(pred_traj, true_traj, markers_on, metric, playerName, dir_name_metric):\n",
    "    dir_name_metric = dir_name + metric + '/'\n",
    "    plt.figure()\n",
    "    plt.plot(pred_traj, marker='o', markevery=markers_on, color='blue', label='Prediction')\n",
    "    plt.plot(true_traj, marker='o', color='red', label='True')\n",
    "    plt.xticks(range(len(true_traj)), range(1, len(true_traj)+1))\n",
    "    plt.legend(loc='best')\n",
    "    plt.xlabel('Years in NBA')\n",
    "    plt.ylabel(metric)\n",
    "    plt.title(playerName + ': ' + metric)\n",
    "    file_name = dir_name_metric + playerName + '.png'\n",
    "    plt.savefig(file_name, bbox_inches='tight')\n",
    "    plt.close()\n",
    "\n",
    "def annual_predictions(playerNames, allPivotedTableDict, donor, pred_interval, metrics_list,\n",
    "                      threshold, donorSetup, denoiseSetup, regression_method, verbose, dir_name):\n",
    "    all_pred = pd.DataFrame()\n",
    "    all_true = pd.DataFrame()\n",
    "    for playerName in playerNames:\n",
    "        # print(playerName)\n",
    "        target = Target(playerName, allPivotedTableDict)\n",
    "        mrsc = mRSC(donor, target, pred_interval, probObservation=1)\n",
    "\n",
    "        player_pred = pd.DataFrame()\n",
    "        player_true = pd.DataFrame()\n",
    "        \n",
    "        for metric in metrics_list:\n",
    "            mrsc.fit_threshold(metric, threshold, donorSetup, denoiseSetup,regression_method, verbose)\n",
    "            pred = mrsc.predict()\n",
    "            true = mrsc.getTrue()\n",
    "            pred.columns = [playerName]\n",
    "            true.columns = [playerName]\n",
    "            player_pred = pd.concat([player_pred, pred], axis=0)\n",
    "            player_true = pd.concat([player_true, true], axis=0)\n",
    "\n",
    "            # plot\n",
    "            pred_list = mrsc.predict().values.flatten()\n",
    "            \n",
    "            for i in range(mrsc.num_k):\n",
    "                metric = mrsc.metrics[i]\n",
    "                true_traj = mrsc.target.data[metric].dropna(axis='columns').values.flatten()\n",
    "                pred_traj = np.dot(mrsc.model.donor_pre.iloc[:, i*mrsc.model.interv_index:(i+1)*mrsc.model.interv_index].T, \n",
    "                               mrsc.model.beta).flatten()\n",
    "                \n",
    "                if mrsc.weighting != None:\n",
    "                    mean_pre = utils.get_preint_data(combinedDF=mrsc.weights[0].to_frame().T,\n",
    "                                                    intervIndex=mrsc.interv_index, totalIndex=mrsc.total_index,\n",
    "                                                    nbrMetrics=mrsc.num_k, reindex=True).values\n",
    "                    var_pre = utils.get_preint_data(combinedDF=mrsc.weights[1].to_frame().T,\n",
    "                                                    intervIndex=mrsc.interv_index, totalIndex=mrsc.total_index,\n",
    "                                                    nbrMetrics=mrsc.num_k, reindex=True).values\n",
    "                    mean_pre = mean_pre[:, i*mrsc.model.interv_index:(i+1)*mrsc.model.interv_index].flatten()\n",
    "                    var_pre = var_pre[:, i*mrsc.model.interv_index:(i+1)*mrsc.model.interv_index].flatten()\n",
    "                    pred_traj = (pred_traj * np.sqrt(var_pre.T)) + mean_pre.T\n",
    "                \n",
    "                pred_traj = np.append(pred_traj, pred_list[i])\n",
    "                markers_on = [true_traj.shape[0]-mrsc.pred_interval]\n",
    "                plot_pred(pred_traj, true_traj, markers_on, metric, playerName, dir_name)\n",
    "    \n",
    "        all_pred = pd.concat([all_pred, player_pred], axis=1)\n",
    "        all_true = pd.concat([all_true, player_true], axis=1)\n",
    "\n",
    "    ###################\n",
    "    # print(all_pred)\n",
    "    print(all_pred.shape)\n",
    "    mask = (all_true !=0 )\n",
    "    mape = np.abs(all_pred - all_true) / all_true[mask]\n",
    "    print(\"*** MAPE ***\")\n",
    "    print(mape.mean(axis=1))\n",
    "    print(\"MAPE for all: \", mape.mean().mean())\n",
    "\n",
    "    rmse = utils.rmse_2d(all_true, all_pred)\n",
    "    print()\n",
    "    print(\"*** RMSE ***\")\n",
    "    print(rmse)\n",
    "    print(\"RMSE for all: \", rmse.mean())\n",
    "    ##############################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** importing data ***\n",
      "*** preparing data ***\n",
      "*** DONE! ***\n"
     ]
    }
   ],
   "source": [
    "\"\"\" USER PARAMETERS \"\"\"\n",
    "starting_year = 1970 \n",
    "min_games_donor = 30\n",
    "min_games_target = 40\n",
    "pred_year = 2015 # the year that we are living in \n",
    "pred_interval = 1 # we are making predictions for pred_year+1 and +2\n",
    "buffer = 4\n",
    "total_stats = [\"PTS\",\"AST\",\"TOV\",\"TRB\",\"STL\",\"BLK\",\"3P\",\"MP\"]\n",
    "normalized_stats = [\"FG%\", \"3P%\", \"FT%\", \"PER\"]\n",
    "\n",
    "\"\"\"\n",
    "import data\n",
    "\"\"\"\n",
    "print(\"*** importing data ***\")\n",
    "players = pd.read_csv(\"../data/nba-players-stats/player_data.csv\")\n",
    "players = players[(players.year_start >= starting_year) & (players.year_start < 2018)]\n",
    "\n",
    "stats = pd.read_csv(\"../data/nba-players-stats/Seasons_Stats.csv\")\n",
    "stats['Player'] = stats['Player'].str.replace('*', '')\n",
    "stats = stats[stats.Player.isin(players.name)]\n",
    "\n",
    "\"\"\" Filter valid players \"\"\"\n",
    "# only look at years in which player played more than 'min_games' games\n",
    "#stats = stats[stats.G >= min_games_donor]\n",
    "\n",
    "# without duplicated names --> to do: how to distinguish multiple player with the same name\n",
    "stats = removeDuplicated(players, stats)\n",
    "stats.Year = stats.Year.astype(int)\n",
    "stats.year_count = stats.year_count.astype(int) \n",
    "\n",
    "print(\"*** preparing data ***\")\n",
    "\n",
    "########### Donor ##########\n",
    "# filter stats by the year\n",
    "stats_donor = stats[stats.Year <= pred_year]\n",
    "stats_donor = stats_donor[stats_donor.G >= min_games_donor] # donors have to have played certain num. years\n",
    "allPivotedTableDict_d, allMetrics = prepareData(stats_donor) # prepareData converts to PTS/G\n",
    "donor = Donor(allPivotedTableDict_d)\n",
    "\n",
    "########### Target ##########\n",
    "# filter stats by the year\n",
    "stats_target = stats[stats.Year <= pred_year+pred_interval]\n",
    "allPivotedTableDict, allMetrics = prepareData(stats_target)\n",
    "\n",
    "\"\"\"\n",
    "targets\n",
    "\"\"\"\n",
    "# targets\n",
    "playerNames = getActivePlayers(stats, pred_year+pred_interval, buffer=buffer, min_games=min_games_target)\n",
    "playerNames.sort()\n",
    "if 'Kevin Garnett' in playerNames: \n",
    "    playerNames.remove(\"Kevin Garnett\")\n",
    "if 'Kobe Bryant' in playerNames:\n",
    "    playerNames.remove(\"Kobe Bryant\")\n",
    "\n",
    "print(\"*** DONE! ***\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** EXPERIMENTAL SECTION ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "predMetrics = [\"PTS_G\",\"AST_G\",\"TOV_G\",\"FG%\",\"FT%\",\"3P_G\",\"TRB_G\",\"STL_G\",\"BLK_G\"]\n",
    "offMetrics = [\"PTS_G\",\"AST_G\",\"TOV_G\",\"PER_w\", \"FG%\",\"FT%\",\"3P_G\"]\n",
    "defMetrics = [\"TRB_G\",\"STL_G\",\"BLK_G\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# *** EXPERIMENT  ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "experiment setup\n",
    "\"\"\"\n",
    "# user input\n",
    "donor_window = 'sliding/'\n",
    "normalize_metric = None \n",
    "threshold = 0.98\n",
    "helper_metrics = ['MP_G', 'PER_w']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plots/mrsc/sliding/no_normalization/MP_G_PER_w_/\n"
     ]
    }
   ],
   "source": [
    "# dir_name\n",
    "if len(helper_metrics):\n",
    "    pred_method = 'mrsc/'\n",
    "else:\n",
    "    pred_method = 'rsc/'\n",
    "\n",
    "if normalize_metric == None:\n",
    "    normalize_metric_label = 'no_normalization/'\n",
    "else:\n",
    "    normalize_metric_label = normalize_metric\n",
    "    \n",
    "threshold_label = str(threshold*100)[:2] + '/'\n",
    "helper_metrics_label = ''\n",
    "for helper_metric in helper_metrics: \n",
    "    helper_metrics_label = helper_metrics_label + helper_metric + '_'\n",
    "helper_metrics_label += '/'\n",
    "\n",
    "dir_name = 'plots/' + pred_method + donor_window + normalize_metric_label + helper_metrics_label\n",
    "print(dir_name)\n",
    "\n",
    "# setup \n",
    "donorSetup= [normalize_metric,\"sliding\", True]\n",
    "denoiseSetup = [\"SVD\", \"all\"]\n",
    "regression_method = \"pinv\"\n",
    "threshold = 0.98\n",
    "verbose = False\n",
    "metrics_list = [[metric] + helper_metrics for metric in predMetrics]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "plots/mrsc/sliding/no_normalization/MP_G_PER_w_/\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "experiment\n",
    "\"\"\"\n",
    "print(\"Computing...\")\n",
    "annual_predictions(playerNames, allPivotedTableDict, donor, pred_interval, metrics_list,\n",
    "                   threshold, donorSetup, denoiseSetup, regression_method, verbose, dir_name) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predMetrics = [\"PTS_G\",\"AST_G\",\"TOV_G\",\"FG%\",\"FT%\",\"3P_G\",\"TRB_G\",\"STL_G\",\"BLK_G\"]\n",
    "offMetrics = [\"PTS_G\",\"AST_G\",\"TOV_G\",\"PER_w\", \"FG%\",\"FT%\",\"3P_G\"]\n",
    "defMetrics = [\"TRB_G\",\"STL_G\",\"BLK_G\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = [predMetrics]\n",
    "#metrics = [[pred_metric] + [] for pred_metric in predMetrics]\n",
    "#metrics = [[pred_metric] + ['PER_w'] for pred_metric in predMetrics]\n",
    "\n",
    "print(metrics)\n",
    "print()\n",
    "\n",
    "player_pred = pd.DataFrame()\n",
    "player_true = pd.DataFrame()\n",
    "\n",
    "for metric in metrics:\n",
    "    #pred_label = list(set(predMetrics) & set(metric))\n",
    "    #print(pred_label)\n",
    "    #print()\n",
    "    mrsc.fit_threshold(metric, threshold, donorSetup, denoiseSetup,regression_method, verbose)\n",
    "    pred = mrsc.predict()\n",
    "    pred = pred[pred.index.isin(predMetrics)]\n",
    "    true = mrsc.getTrue()\n",
    "    pred.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "    true.columns = [playerName+\" \"+ str(a) for a in range(pred_interval)]\n",
    "    player_pred = pd.concat([player_pred, pred], axis=0)\n",
    "    player_true = pd.concat([player_true, true], axis=0)\n",
    "    print(player_pred)\n",
    "    print()\n",
    "\n",
    "player_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>PTS_G</th>\n",
       "      <td>27.8184</td>\n",
       "      <td>25.1594</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             0        1\n",
       "PTS_G  27.8184  25.1594"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# debug\n",
    "playerName = playerNames[0]\n",
    "playerName = 'James Harden'\n",
    "metrics_list = [[metric] for metric in predMetrics]\n",
    "metric = metrics_list[0]\n",
    "\n",
    "# print(playerName)\n",
    "target = Target(playerName, allPivotedTableDict)\n",
    "mrsc = mRSC(donor, target, pred_interval, probObservation=1)\n",
    "\n",
    "mrsc.fit_threshold(metric, threshold, donorSetup, denoiseSetup,regression_method, verbose)\n",
    "\n",
    "# plot\n",
    "pred_list = mrsc.predict()\n",
    "pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15787     9.907895\n",
       "16412    12.170732\n",
       "16975    16.838710\n",
       "17511    25.935897\n",
       "18116    25.356164\n",
       "18710    27.370370\n",
       "19303    28.975610\n",
       "19874    29.086420\n",
       "dtype: float64"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stats[stats.Player == playerName]['PTS']/stats[stats.Player == playerName]['G']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['PTS_G', 'MP_G']"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i = 0\n",
    "metric = mrsc.metrics[i]\n",
    "true_traj = mrsc.target.data[metric].dropna(axis='columns').values.flatten()\n",
    "pred_traj = np.dot(mrsc.model.donor_pre.iloc[:, i*mrsc.model.interv_index:(i+1)*mrsc.model.interv_index].T, \n",
    "                   mrsc.model.beta).flatten()\n",
    "\n",
    "mean_pre = utils.get_preint_data(combinedDF=mrsc.weights[0].to_frame().T,\n",
    "                                intervIndex=mrsc.interv_index, totalIndex=mrsc.total_index,\n",
    "                                nbrMetrics=mrsc.num_k, reindex=True)\n",
    "\n",
    "var_pre = utils.get_preint_data(combinedDF=mrsc.weights[1].to_frame().T,\n",
    "                                intervIndex=mrsc.interv_index, totalIndex=mrsc.total_index,\n",
    "                                nbrMetrics=mrsc.num_k, reindex=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for individual player prediction \n",
    "def getNormalizedMetric(stats, metric, newColName):\n",
    "    \"\"\"\n",
    "    stats: (df) stats dataframe\n",
    "    metric: (string) column name of stats df\n",
    "    newColName: (string) new column name for the processed data\n",
    "    \n",
    "    output: (df) for each player, for each year, the per-game metric is computed.\n",
    "    \"\"\"\n",
    "    columnsOfInterest = [\"year_count\", \"Player\", \"G\", metric]\n",
    "    df = stats.loc[:,columnsOfInterest].groupby([\"Player\",\"year_count\"]).sum()\n",
    "    df[newColName] = df[metric]/df[\"G\"]\n",
    "    return df.iloc[:,-1:]\n",
    "\n",
    "def getNormalizedMetricDict(stats, metrics):\n",
    "    \"\"\"\n",
    "    stats: (df) stats dataframe\n",
    "    metrics: (list) column names (strings) of stats df\n",
    "    \n",
    "    output: (dict) dict of df's.\n",
    "    \"\"\"\n",
    "    metricsPerGameDict = {}\n",
    "    for metric in metrics:\n",
    "        newColName = metric+\"_G\"\n",
    "        metricsPerGameDict.update({newColName : getNormalizedMetric(stats, metric, newColName)})\n",
    "    return metricsPerGameDict\n",
    "\n",
    "\n",
    "def prepareData_(stats, total_stats, normalized_stats):\n",
    "    # convert 'total' statistics to 'per game' statistics -> dictionary\n",
    "    metricsConvertedDict = getConvertedMetricsPerGameDict(stats, total_stats)\n",
    "    \n",
    "    # convert 'per game' statistics to dictionary\n",
    "    metricsNormalizedDict = getNormalizedMetricsDict(stats, normalized_stats)\n",
    "    \n",
    "    allMetricsDict = {**metricsConvertedDict, **metricsNormalizedDict}\n",
    "    allPivotedTableDict = getPivotedTableDict(allMetricsDict)\n",
    "    return allPivotedTableDict\n",
    "    \n",
    "    \n",
    "    \"\"\"# transform stats to a dictionary composed of df's for each stat\n",
    "    # the stats are re-calculated to get one stat for each year\n",
    "    metricsPerGameColNames = [\"PTS\",\"AST\",\"TOV\",\"TRB\",\"STL\",\"BLK\",\"3P\",\"MP\"]\n",
    "    metricsPerGameDict = getMetricsPerGameDict(stats, metricsPerGameColNames)\n",
    "\n",
    "    metricsPerCentColNames = [\"FG\",\"FT\"]\n",
    "    metricsPerCentDict = getMetricsPerCentDict(stats, metricsPerCentColNames)\n",
    "\n",
    "    metricsWeightedColNames = [\"PER\"]\n",
    "    metricsWeightedDict = getMetricsWeightedDict(stats, metricsWeightedColNames)\n",
    "\n",
    "    allMetricsDict = {**metricsPerGameDict, **metricsPerCentDict, **metricsWeightedDict}\n",
    "    allPivotedTableDict = getPivotedTableDict(allMetricsDict)\n",
    "    allMetrics = list(allMetricsDict.keys())\n",
    "    return allPivotedTableDict, allMetrics\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
